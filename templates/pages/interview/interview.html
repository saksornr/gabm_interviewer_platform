{% extends "pages/interview/interview_base.html" %}
{% load static %}

{% block custom_head_content %}
<style>
  body {
    margin: 0;
    padding: 0;
    display: flex;
    justify-content: center;
    align-items: center;
    height: 100vh;
    background: linear-gradient(45deg, #6e45e2, #88d3ce);
  }

  button {
    padding: 10px 20px;
    border: none;
    border-radius: 5px;
    background-color: #fff;
    color: #333;
    font-size: 16px;
    cursor: pointer;
    margin: 10px;
    transition: background-color 0.3s, transform 0.2s;
  }

  button:hover {
    background-color: #f8f9fa;
    transform: scale(1.05);
  }

  button:active {
    transform: scale(0.95);
  }

  @keyframes flashGreen {
    0%, 100% {
      box-shadow: 0 0 15px rgba(255, 255, 255, 0.6), 
                  0 0 25px rgba(255, 255, 255, 0.6), 
                  0 0 35px rgba(255, 255, 255, 0.6);
    }
    50% {
      box-shadow: 0 0 15px rgba(0, 255, 0, 0.8), 
                  0 0 25px rgba(0, 255, 0, 0.8), 
                  0 0 35px rgba(0, 255, 0, 0.8);
    }
  }

  .flash-animation {
    animation: flashGreen 6s ease-in-out;
  }

  #visualizer {
    visibility: hidden;
    position: relative;
    width: 180px;
    height: 180px;
    background-color: #fff;
    border-radius: 50%;
    border: 2px solid #fff;
    position: absolute;
    left: 50%;
    top: 38%;
    transform: translate(-50%, -50%) scale(1);
    transition: transform 0.1s ease;
    box-shadow: 0 0 15px rgba(255, 255, 255, 0.6), 0 0 25px rgba(255, 255, 255, 0.6), 0 0 35px rgba(255, 255, 255, 0.6);
    display: flex;
    justify-content: center;
    align-items: center;
  }

  #micIcon, #speakerIcon, #thinkingIcon {
    position: absolute;
    top: 38%;
    left: 50%;
    transform: translate(-50%, -50%);
  }

  #speakerIcon{ 
    width:2.5em;
  }

  #micIcon{ 
    font-size:1.8em; 
  }

  #visualizer>i {
    position: absolute;
    font-size: 2em;
    color: rgb(110, 129, 255);
  }

  #controls {
    position: fixed;
    bottom: 70px;
    left: 50%;
    transform: translateX(-50%);
  }

  #endButton {
    display: none;
    opacity: 0; /* Start with the button invisible */
    transition: opacity 2s; /* Gradual fade effect over 2 seconds */
    margin-bottom: 0.7em;
  }

  #interviewerTranscriptContainer {
    display:none; 
    width:400px; 
    margin: 0 auto;  
    padding:1.8em; 
    color:white; 
    border-radius: 1.5em; 
    margin-bottom: 1em;
    max-height: 108px; 
    overflow-y: scroll; 
    text-align: left;
  }

  #refresh {
    color: rgba(255, 255, 255, 1);
    font-weight: 900;
  }

  #refresh:hover {
    color: rgba(255, 255, 255, 1);
    font-weight: 600;
  }

  /* This is CSS specifically for audio play animation when the agent is
    thinking about its next lines */
  #thinkingIcon {
    display: none;
    justify-content: center;
    align-items: center;
    height: 100vh;
    opacity: 0; /* Make it invisible initially */
    transition: opacity 2s; /* Adjust time as needed */
  }

  .circle {
    width: 24px;
    height: 24px;
    margin: 0 10px;
    background-color: white; /* You can change the color */
    border-radius: 50%;
    animation: wave 1.5s ease-in-out infinite;
    box-shadow: 0 0 15px rgba(255, 255, 255, 0.6), 0 0 25px rgba(255, 255, 255, 0.6), 0 0 35px rgba(255, 255, 255, 0.6);
  }

  .circle:nth-child(2) {
    animation-delay: 0.1s;
  }

  .circle:nth-child(3) {
    animation-delay: 0.2s;
  }

  @keyframes wave {
    0%, 100% {
      transform: translateY(0);
    }
    50% {
      transform: translateY(-20px);
    }
  }

  /* Now for the instructions */
  #interaction_wrapper {
    position: absolute;
    z-index:100;  
  }

  /* For the instruction */
  #instructions {
    background-color: white; 
    padding:3em; 
    border-radius: 2em; 
    box-shadow: 0 0 15px rgba(255, 255, 255, 0.22), 0 0 25px rgba(255, 255, 255, 0.22), 0 0 35px rgba(255, 255, 255, 0.22);
  }

  #instructions h1 {
    font-size:1.5em
  }

  #instructions p {
    font-size:1.1em; 
    margin-top:1.8em
  }

  #instructions ul {
    font-size:1.1em
  }

  /* Now for the progress bar */
  #progressBarWrapper {
    visibility: hidden;
    margin-bottom: 2em;
    border:none;
  }

  #progressBar {
    position: relative;
    padding: 10px;
    border-bottom:solid;
    background-image: url("{% static 'gabm/img/walk2.png' %}");
    background-size: auto 110%;
    background-position: center center; 
    width:400px;
    height:72px;
    border-radius: 20px;
    border:none;
  }

  #avatar {
    position: absolute;
    left: 1%;
    width: 32px;
    top: 50%;
    transform: translateY(-50%);
    transition: left 5s ease; 
  }

  /* Styles for screens with a height less than 600px */
  @media screen and (max-height: 599px) {
      #micIcon, #speakerIcon, #thinkingIcon, #visualizer {
          top: 20%;
      }
  }

  /* Styles for screens with a height between 600px and 799px */
  @media screen and (min-height: 600px) and (max-height: 799px) {
      #micIcon, #speakerIcon, #thinkingIcon, #visualizer {
          top: 30%;
      }
  }

  /* Styles for screens with a height of 800px or more */
  @media screen and (min-height: 800px) {
      #micIcon, #speakerIcon, #thinkingIcon, #visualizer {
          top: 38%;
      }
  }

  #savedMessage {
    display: none; 
    opacity: 0; 
    transition: opacity 2s;
    color: rgba(0, 255, 0, 0.8);
    font-weight: 900;
    font-size: 1.1em;
  }
</style>
{% endblock custom_head_content %}

{% block content %}
<div id="interaction_wrapper" class="row" style="">
  <div class="col-lg-3 col-md-3 col-sm-2 col-0"></div>
  <div class="col-lg-6 col-md-6 col-sm-8 col-12" style="padding:2em">
    <div id="instructions">
      <h1>
        <em style="">Are you ready?</em>
      </h1>
      <p>
        Please note:
      </p>
      <ul>
        <li>Ensure your speaker and microphone are turned on. If your browser prompts you to enable them, please do so.</li>
        <li>You can start speaking when you see this icon: <strong><i class="fa fa-microphone" style=""></i></strong></li>
        <li>A white circle at the center of the screen will enlarge when it detects your voice. It will gradually fade if you stop talking.</li>
        <li>Your progress will be indicated by a circular ring that will gradually grow around Isabella's sprite avatar. If it becomes a full circle, you are done!</li>
      </ul><br>
      <div style="text-align:center; z-index:100 !important">
        <button id="startButton" class="btn rounded-pill btn-info" style="z-index:100 !important"><strong>Click here to start the interview</strong></button>
      </div>
    </div>
  </div>

  <div id="controls" style="width:100%; text-align:center; display: none">

    <div id="interviewerTranscriptContainer" style="">
      <p id="interviewerTranscript" style=""></p>
    </div>

    {% if user_avatar.front_gif %}
    <div class="row" id="progressBarWrapper">
      <div class="col-lg-3 col-md-3 col-sm-2 col-0"></div>
      <div class="col-lg-6 col-md-6 col-sm-8 col-12" >
        <div id="progressBar" style="margin: 0 auto;">
          <img id="avatar" src="{{ user_avatar.right_gif.url }}" alt="Front Avatar" style="">
        </div>
      </div>
    </div>
    {% endif %}
    
    <div id="interviewControls" style="display: none; text-align:center; ">    
      
      <button id="buttonShowTranscript" class="btn rounded-pill btn-info" style="border-width:2px"><strong id="buttonShowTranscriptInner">Show Isabella's subtitles</strong></button>

      <button id="buttonPause" class="btn btn-icon rounded-pill btn-outline-facebook" 
        data-bs-toggle="modal" data-bs-target="#modals-pause" 
        style="border-width:5px; color: rgba(255, 255, 255, 0.85);"><strong><i class="fa-solid fa-pause"></i></strong></button>

      <button id="buttonInfo" class="btn btn-icon rounded-pill btn-outline-facebook" 
        data-bs-toggle="modal" data-bs-target="#modals-info" 
        style="border-width:5px"><strong><i class="fa-solid fa-info"></i></strong></button>

    </div>
    <a href="{% url 'home' %}" id="endButton" type="button" class="btn rounded-pill btn-info"><strong>Click here to return to the main page</strong></a>

    <div id="refreshContainer" style="width: 400px;  margin: 0 auto; visibility: hidden; border-radius: 20px; background-color: red; margin-top:1.2em; padding:0.2em; box-shadow: 0 0 15px rgba(255, 255, 255, 0.22), 0 0 25px rgba(255, 255, 255, 0.22), 0 0 35px rgba(255, 255, 255, 0.22);">
        <a id="refresh" data-bs-toggle="modal" data-bs-target="#modals-refresh" >
          Help! Isabella froze for 60+ seconds.
        </a>
    </div>
  </div>
</div>

<div id="container" style="">
  <div id="visualizer">
    <img id="progressImage" src="{% static 'gabm/img/progress_arcs_thick/0.png' %}" style="width:17em; visibility: hidden;" />
  </div>
  <i id="micIcon" class="fa fa-microphone" style="display: none"></i>
  <img id="speakerIcon" src="{% static 'gabm/img/Isabella_Rodriguez.png' %}" style="display: none" />
  <span id="thinkingIcon" style="display: none">
    <div class="circle"></div>
    <div class="circle"></div>
    <div class="circle"></div>
  </span>

  <br>
  <br>
  <span id="savedMessage">[ Progress saved ]</span>
</div>
{% endblock content %}

{% block modal_block %}
  {% include "pages/interview/interview_modals.html" %}
{% endblock modal_block %}

{% block js_content %}
<script>
  // #####################################################################
  // [SECTION 1: SETTING VARIABLES]

  // Handles for HTML elements:
  let visualizerElement = document.getElementById('visualizer');
  let progressImage = document.getElementById('progressImage');
  let micIcon = document.getElementById('micIcon');
  let speakerIcon = document.getElementById('speakerIcon');

  // Web Audio API objects. 
  let audioContext;
  let agentAnalyser, userAnalyser; 
  let userProcessor;
  let mediaRecorder;

  // Settings and parameters: 
  const speechThreshold = {{ request.user.audio_calibration_float }};
  const maxSilenceDuration = 6;
  const fade_exponent = 3;
  const recentRMSsSizes = 10;
  const RPTStartThreshold = 3;
  // const RPTStartThreshold = 1;
  const RPTEndThreshold = 5; 

  // State variables: 
  let silenceStarted;
  let isUserTurn = false;
  let userHasSpoken = false;
  let currRMS;
  let recentRMSs = []
  let currentAudioElement = null;
  let audioChunks = [];
  let isLoading = false;
  let startUtteranceTime;
  let mediaSilenceResetTimer = 0;
  let realMediaOnStop = false;

  // Timers: 
  let fadeTimer; 


  // #####################################################################
  // [SECTION 2: HELPER FUNCTIONS]

  /**
   * Retrieves a cookie value by its name.
   * @param {string} name The name of the cookie.
   * @return {?string} The value of the cookie, or null if not found.
   */
  function getCookie(name) {
    let cookieValue = null;
    if (document.cookie && document.cookie !== '') {
      const cookies = document.cookie.split(';');
      for (let i = 0; i < cookies.length; i++) {
        const cookie = cookies[i].trim();
        // Check if the cookie string begins with the name we want.
        if (cookie.substring(0, name.length + 1) === (name + '=')) {
          const cookiePart = cookie.substring(name.length + 1);
          cookieValue = decodeURIComponent(cookiePart);
          break;
        }
      }
    }
    return cookieValue;
  }
  const csrftoken = getCookie('csrftoken');


  /**
   * Calculates the file size in bytes from a Base64 string.
   * @param {string} base64String The Base64 encoded string.
   * @return {number} The calculated file size in bytes.
   */
  function calculateBase64FileSize(base64String) {
    // Remove header from data URL, if present
    let base64WithoutHeader = base64String.split(',')[1] || base64String;

    // Adjust for padding
    let padding = 0;
    if (base64WithoutHeader.endsWith('==')) {
      padding = 2;
    } else if (base64WithoutHeader.endsWith('=')) {
      padding = 1;
    }

    // Calculate the total number of bits in the base64 string (6 bits per
    // character)
    let totalBits = base64WithoutHeader.length * 6;
    // Subtract the padding bits (8 bits per '=' character)
    let adjustedBits = totalBits - padding * 8;
    // Convert bits to bytes
    let fileSizeInBytes = adjustedBits / 8;

    return fileSizeInBytes;
  }


  // #####################################################################
  // [SECTION 3: DECORATION]

  /**
   * Updates the icons' visuals based on the current stage.
   */
  function updateIcons() {
    if (isLoading) {
      // <Part 1. Icon when the agent is thinking of the next line>
      visualizer.style.backgroundColor = 'transparent';
      visualizer.style.borderColor = 'transparent';
      visualizer.style.boxShadow = "none";
      progressImage.style.display = 'none';

      micIcon.style.display = 'none';
      speakerIcon.style.display = 'none';
      thinkingIcon.style.display = 'flex';
      thinkingIcon.style.opacity = 1;

      // Fade in for the thinking animation
      var loadingContainer = document.querySelector('#thinkingIcon');
      loadingContainer.style.display = 'none';

      function fadeInLoading() {
        if (fadeTimer) {
          clearTimeout(fadeTimer);
        }
        loadingContainer.style.display = 'flex'; // Set display to flex
        fadeTimer = setTimeout(() => {
          loadingContainer.style.opacity = 1; // Start the fade in
        }, 15); // Timeout allows CSS to recognize the display change
      }
      fadeInLoading();

    } else if (isUserTurn) {
      // Icon for when the user is speaking
      visualizer.style.backgroundColor = 'white';
      visualizer.style.borderColor = 'white';
      visualizer.style.boxShadow = '0 0 15px rgba(255, 255, 255, 0.6), ' +
                                   '0 0 25px rgba(255, 255, 255, 0.6), ' +
                                   '0 0 35px rgba(255, 255, 255, 0.6)';
      progressImage.style.display = 'flex';
      micIcon.style.display = 'block';
      speakerIcon.style.display = 'none';
      thinkingIcon.style.display = 'none';

      document.getElementById('refreshContainer').style.visibility = 'hidden';

    } else {
      // Icon for when the agent is speaking
      visualizer.style.backgroundColor = 'white';
      visualizer.style.borderColor = 'white';
      visualizer.style.boxShadow = '0 0 15px rgba(255, 255, 255, 0.6), ' +
                                   '0 0 25px rgba(255, 255, 255, 0.6), ' +
                                   '0 0 35px rgba(255, 255, 255, 0.6)';
      progressImage.style.display = 'flex';
      micIcon.style.display = 'none';
      speakerIcon.style.display = 'block';
      thinkingIcon.style.display = 'none';

      document.getElementById('refreshContainer').style.visibility = 'hidden';
    }
  }


  // #####################################################################
  // [SECTION 3: CORE FUNCTIONS]

  /**
   * Returns the number of recent RMS values that exceeded the speech 
   * threshold.
   * @param {number} RMS The current RMS value.
   * @return {number} The count of recent RMS values exceeding the 
   * threshold.
   */
  function getRecentPassedThreshold(RMS) {
    // If RMS is higher than the speechThreshold, we add 1 to the 
    // recentRMSs, which is a list of floats (RMSs) with the length of
    // recentRMSsSizes. If it is lower or equal, we add 0. 
    let passedThreshold = 0;
    if (RMS > speechThreshold) {
      // console.log(RMS + " :: "  + speechThreshold);
      passedThreshold += 1;
    }
    recentRMSs.push(passedThreshold);

    // Remove the oldest entry if there are more than 20 items
    if (recentRMSs.length > recentRMSsSizes) {
      recentRMSs.shift(); 
    }

    // Sum up the values using reduce
    const recentPassedThreshold = recentRMSs.reduce(
      (accumulator, currentValue) => accumulator + currentValue, 0
    ); 
    return recentPassedThreshold
  }

  /**
   * Calculates the Root Mean Square (RMS) of the audio input.
   * @param {AudioProcessingEvent} audioProcessingEvent The audio 
   * processing event.
   */
  function calculateRMS(audioProcessingEvent) {
    const inputBuffer = audioProcessingEvent.inputBuffer;
    const inputData = inputBuffer.getChannelData(0);
    let sumSquares = 0.0;
    for (let i = 0; i < inputData.length; i++) {
      sumSquares += inputData[i] * inputData[i];
    }
    currRMS = Math.sqrt(sumSquares / inputData.length);
  }

  /**
   * Handles the beginning and end of speech based on RMS thresholds.
   * @param {number} recentPassedThreshold The recent passed threshold 
   * value.
   */
  function handleSpeechBeginAndEnd(recentPassedThreshold) {
    // console.log(mediaSilenceResetTimer);
    if (mediaRecorder.state === "inactive") {
      mediaRecorder.start();
      mediaSilenceResetTimer = 0;
    }

    if (mediaRecorder.state === "recording" && !userHasSpoken){
      mediaSilenceResetTimer += 1
      if (mediaSilenceResetTimer % 150 === 0) {
        realMediaOnStop = false;
        mediaRecorder.stop();
        mediaRecorder.start();
        mediaSilenceResetTimer = 0;
      }
    }

    if (recentPassedThreshold > RPTStartThreshold && !userHasSpoken) {
      userHasSpoken = true;
      startUtteranceTime = new Date()
      // console.log("STARTING MEDIA RECORDER")
      // mediaRecorder.start();
    }
    if (userHasSpoken) {
      if (currRMS <= speechThreshold) {
        silenceStarted = silenceStarted || new Date();
      } else if (recentPassedThreshold < RPTEndThreshold) {
        silenceStarted = false;
      }
      // 600000 stands for 10 minutes... So we want to end if it's been 10 minutes... 
      if ((silenceStarted && (new Date() - silenceStarted > maxSilenceDuration * 1000)) ||
        (new Date() - startUtteranceTime > 600000)) {
        isLoading = true;
        isUserTurn = false;
        userHasSpoken = false;
        silenceStarted = false;
        realMediaOnStop = true;
        mediaRecorder.stop();
      }
    }
  }

  /**
   * Calculates the duration of silence.
   * @return {number} The duration of silence in seconds.
   */
  function calculateSecSilence() {
    if (!silenceStarted) {
      return 0;
    }
    const currTime = new Date();
    return (currTime - silenceStarted) / 1000;
  }

  /**
   * Applies a fade away effect based on the duration of silence.
   * @param {number} secSilence The duration of silence in seconds.
   */
  function applyFadeAwayEffect(secSilence) {
    let silenceRatio = secSilence / maxSilenceDuration;
    let fadeFactor = Math.pow(silenceRatio, fade_exponent);
    let opacity = Math.max(1 - fadeFactor, 0);
    opacity = opacity < 0.01 ? 0 : opacity;
    visualizerElement.style.opacity = opacity;
    micIcon.style.opacity = opacity;
  }

  function checkLoadingStatus() {
    let loadingTimer = null;

    const interval = setInterval(() => {
        if (isLoading) {
            if (loadingTimer === null) {
                loadingTimer = setTimeout(() => {
                    document.getElementById('refreshContainer').style.visibility = 'visible';
                    clearInterval(interval); // Stop the interval
                    clearTimeout(loadingTimer); // Clear the timeout to clean up
                    loadingTimer = null; // Reset the timer variable
                }, 52000); // 52 seconds
            }
        } else {
            clearTimeout(loadingTimer); // Clear the timeout if isLoading becomes false
            loadingTimer = null;
        }
    }, 5000); // Check every 5 seconds
  }



  /**
   * Animates the agent and interviewee's voice, and records the 
   * interviewee's voice. 
   */
  function animateAndRecord() {
    window.requestAnimationFrame(animateAndRecord);
    let analyser = isUserTurn ? userAnalyser : agentAnalyser;
    
    // Frequency data handling
    let dataArray = new Uint8Array(analyser.frequencyBinCount);
    analyser.getByteFrequencyData(dataArray);

    // Calculation of average frequency and RMS
    let sum = dataArray.reduce((a, b) => a + b, 0);
    let average = sum / dataArray.length;
    let scale = 1 + (average / 64);

    // Apply the scale transformation to the visualizer element. But this
    // is only if the agent is speaking, or if we have determined that the
    // user started speaking. 
    if (!isUserTurn || userHasSpoken) {
      let transformValue = `translate(-50%, -50%) scale(${scale})`;
      visualizerElement.style.transform = transformValue;
    }

    // <User's turn handler>
    // If this is the user's turn, we do a couple of things here: 
    // 1) Make the visualizer circle gradually fade away as the user stops
    //    speaking
    // 2) Start recording if we detect that the user started to talk, and 
    //    end the recording if we determine that they are done. 
    if (isUserTurn) {
      userProcessor.onaudioprocess = calculateRMS;

      const recentPassedThreshold = getRecentPassedThreshold(currRMS);
      handleSpeechBeginAndEnd(recentPassedThreshold);

      // Fade away effect
      let secSilence = calculateSecSilence();
      applyFadeAwayEffect(secSilence);
    }

    // Update any icons or graphics
    updateIcons();

    // checkLoadingStatus();
  }


  function showSavedMessage() {
      var message = document.getElementById('savedMessage');
      message.style.display = 'block';
      message.style.opacity = '1';

      // Start fading out after 5 seconds
      setTimeout(function() {
          message.style.opacity = '0';
      }, 6000);

      // Set display none after transition is complete (2 seconds for fade out + 5 seconds delay)
      setTimeout(function() {
          message.style.display = 'none';
      }, 8000);
  }


  

  // #####################################################################
  /**
   * Plays the audio sound from a given audio element.
   * @param {HTMLAudioElement} newAudioElement The audio element to play.
   */
  // function playAudio(newAudioElement, curr_data) {
  //   // Create the audio source and connect to the analyser
  //   let agentSource = audioContext.createMediaElementSource(newAudioElement);
  //   agentSource.connect(agentAnalyser);
  //   agentAnalyser.connect(audioContext.destination);

  //   // Play logic based on the state of current audio
  //   if (currentAudioElement && !currentAudioElement.ended) {
  //     currentAudioElement.onended = () => {
  //       // Play the new audio element
  //       newAudioElement.play();
  //       // Update the interviewer transcript container with the current data
  //       document.getElementById('interviewerTranscriptContainer').innerHTML = "<strong>Isabella:</strong> " + curr_data["interviewer_transcript"];
  //     };
  //   } else {
  //     newAudioElement.play();
  //     document.getElementById('interviewerTranscriptContainer').innerHTML = "<strong>Isabella:</strong> " + curr_data["interviewer_transcript"];
  //   }
  //   currentAudioElement = newAudioElement;
  // }

  let agentSource = null; // Global or higher scope variable to reuse the source node

  function playAudio(newAudioElement, curr_data) {
      // Check if the agentSource already exists and disconnect it if so
      // if (agentSource) {
      //     agentSource.disconnect();

      // }

    agentSource = audioContext.createMediaElementSource(newAudioElement);
    agentSource.mediaElement = newAudioElement;
      // Assuming audioContext and agentAnalyser are already defined in your scope
      try {
          // Attempt to reuse the existing source node with a new media element
          // Note: This operation is not supported by all browsers, and in practice,
          // you may need to create a new source node for each new media element.
          agentSource.mediaElement = newAudioElement;
      } catch (e) {
          // If reusing the source node is not possible, create a new one
          // agentSource = audioContext.createMediaElementSource(newAudioElement);
      }
      
      // Connect the source node to the analyser and the destination
      agentSource.connect(agentAnalyser);
      agentAnalyser.connect(audioContext.destination);

      // Replace the play logic as before
      if (currentAudioElement && !currentAudioElement.ended) {
          currentAudioElement.onended = () => {
              newAudioElement.play();
              document.getElementById('interviewerTranscriptContainer').innerHTML = "<strong>Isabella:</strong> " + curr_data["interviewer_transcript"];
          };
      } else {
          newAudioElement.play();
          document.getElementById('interviewerTranscriptContainer').innerHTML = "<strong>Isabella:</strong> " + curr_data["interviewer_transcript"];
      }
      currentAudioElement = newAudioElement;
  }





  /**
   * Displays the end button.
   */
  function showEndButton() {
    document.getElementById('interviewControls').style.display = 'none';
    document.getElementById('refreshContainer').style.display = 'none';

    var button = document.getElementById("endButton");
    button.style.display = "inline-flex";
    setTimeout(() => button.style.opacity = 1, 10);
  }

  /**
   * Handles the completion of the interview.
   */
  function handleInterviewCompletion() {
    if (!currentAudioElement || currentAudioElement.ended) {
      audioContext.close();
      showEndButton();
    } else {
      currentAudioElement.onended = () => {
        audioContext.close();
        showEndButton();
      };
    }
  }

  /**
   * Handles the user's media stream.
   * @param {MediaStream} stream The media stream to process.
   */
  function handleUserMedia(stream) {
    const userSource = audioContext.createMediaStreamSource(stream);
    userProcessor = audioContext.createScriptProcessor(4096, 1, 1);
    userSource.connect(userAnalyser);
    userSource.connect(userProcessor);
    userProcessor.connect(audioContext.destination);

    let mimeType = 'audio/webm';  // Default mimeType
    if (MediaRecorder.isTypeSupported('audio/mp4')) {
      mimeType = 'audio/mp4';  // Use MP4 if supported
    }

    // Initializing MediaRecorder
    mediaRecorder = new MediaRecorder(stream, { mimeType: mimeType });

    // Push audio data into chunks array when data is available
    mediaRecorder.ondataavailable = event => {
      audioChunks.push(event.data);
    };

    // Behavior of mediaRecorder upon stopping
    mediaRecorder.onstop = () => {
      if (realMediaOnStop) {
        const audioBlob = new Blob(audioChunks, { 'type' : mimeType });

        const audioUrl = URL.createObjectURL(audioBlob);
        const audio = new Audio(audioUrl);

        // Convert Blob to base64 and include in newData
        const reader = new FileReader();
        reader.readAsDataURL(audioBlob); 
        reader.onloadend = function() {
          const base64data = reader.result;
          var newData = {
            started: false, 
            user_utt: base64data, 
            script_v: "{{script_v}}", 
            mime_type: mimeType
          };

          // Log file size in bytes
          const fileSizeInBytes = calculateBase64FileSize(base64data);
          // console.log("File size in bytes:", fileSizeInBytes);

          audio.onended = () => {
            URL.revokeObjectURL(audioUrl); // Revoke the blob URL after audio has ended
          };

          // Recursive call with new or modified data
          take_one_step(newData); 
        }
      } else {
        audioChunks = [];
      }
    };
  }

  function animateAvatarToPosition(percentage) {
    if (percentage < 0 || percentage > 100) {
        // console.error("Percentage should be between 0 and 100");
        return;
    }

    var avatar = document.getElementById("avatar");
    avatar.style.left = percentage + "%";
  }


  /**
   * This is the main recursive loop for the interviewer agent.
   * @param {Object} data The data to be processed in each step.
   */
  function take_one_step(data) {
    // POST call to send data from frontend to backend server
    fetch('{% url "handler_take_one_step" %}', {
      method: 'POST',
      body: JSON.stringify(data),
      headers:{
        'Content-Type': 'application/json',
        'X-CSRFToken': csrftoken
      }
    })
    .then(response => response.json())
    .then(data => {
      // Update progress image
      // console.log(data)
      if (data["progress_circle_rad"] !== undefined) {
        const progressImage = document.getElementById('progressImage');
        progressImage.src = data.progress_circle_url;
        // console.log("fin_percent: " + data["fin_percent"]);
        animateAvatarToPosition(data["fin_percent"]);
      }

      if (data["interview_completed"] == true) {
        // Completing the interview
        handleInterviewCompletion();
      } else {
        // Handling ongoing interview process
        // Playing the interviewer agent's voice audio
        isLoading = false;
        let audio_path = data["audio_url"];
        let audioElement = new Audio(audio_path);
        audioElement.crossOrigin = "anonymous";
        if (audio_path) { 
          playAudio(audioElement, data);
        }
        animateAndRecord();

        if (data["module_completed"]) {
            // Add the animation class
          // Call this function when you want to show the message
          showSavedMessage();

            visualizerElement.classList.add('flash-animation');
            
            // Optional: Remove the class after the animation ends
            visualizerElement.addEventListener('animationend', function() {
              visualizerElement.classList.remove('flash-animation');
            });
        }

        // Record the user response
        if (!data["skip_user_utt"]) {
          audioElement.onended = function() {
            isUserTurn = true; 
            userHasSpoken = false;ã€€

            audioChunks = [];

            if (navigator.mediaDevices.getUserMedia) {
              navigator.mediaDevices.getUserMedia ({audio: true})
              .then(handleUserMedia)
              .catch(err => console.log('gUM error: ' + err));
            } else {
              console.log('getUserMedia not supported on your browser!');
            }
          }
        } else {
          var newData = {
            started: false, 
            user_utt: null, 
            script_v: "{{script_v}}", 
            mime_type: null
          };
          take_one_step(newData); 
        }
      };
    })
    .catch(error => console.error('Error:', error));
  }


  // #####################################################################
  // [SECTION 5: DEFINING THE START BUTTON]
  // Defining the start button's behavior
  document.getElementById('startButton')
    .addEventListener('click', function() {
    // Hide the start button and instructions once clicked
    document.getElementById('instructions').style.display = 'none';
    document.getElementById('controls').style.display = 'block';
    
    this.style.display = "none";

    // Hide the start button and instructions once clicked
    document.getElementById('interviewControls').style.display = 'block';
    document.getElementById('refreshContainer').style.visibility = 'hidden';
    document.getElementById('visualizer').style.visibility = 'visible';
    document.getElementById('progressBarWrapper').style.visibility = 'visible';

    // Setup of Web Audio API variables
    if (!audioContext && navigator.mediaDevices.getUserMedia) {
      const AudioCtx = window.AudioContext || window.webkitAudioContext;
      audioContext = new AudioCtx();
      agentAnalyser = audioContext.createAnalyser();
      agentAnalyser.fftSize=2048
      userAnalyser = audioContext.createAnalyser();
      userAnalyser.fftSize=2048
    }

    // Data for the first interaction with the backend
    let data = {
      started: true, 
      user_utt: null, 
      script_v: "{{script_v}}", 
      mime_type: null
    };
    // Start the recursive calls
    take_one_step(data); 
  });


  document.getElementById('buttonShowTranscript')
    .addEventListener('click', function() {
      var transcriptContainer = document.getElementById('interviewerTranscriptContainer');
      if (transcriptContainer.style.display === 'block') {
          transcriptContainer.style.display = 'none';
          document.getElementById('buttonShowTranscriptInner').innerHTML = "Show Isabella's subtitles";
      } else {
          transcriptContainer.style.display = 'block';
          document.getElementById('buttonShowTranscriptInner').innerHTML = "Hide Isabella's subtitles";
      }
  });
</script>
{% endblock js_content %}






























